# Stochastic Differential Equations


Stochastic differential equations (SDEs) are differential equations driven by **Brownian motion**.  
They provide the canonical way to define **diffusion processes** and connect stochastic calculus to PDE, generators, and change of measure.

Throughout, let $(\Omega,\mathcal{F},\{\mathcal{F}_t\},\mathbb{P})$ be a filtered probability space carrying a Brownian motion $W_t$.

---

## From Itô Integrals to SDEs


An **Itô process** has the form

$$
X_t = X_0 + \int_0^t b(s,X_s)\,ds + \int_0^t \sigma(s,X_s)\,dW_s.
$$

Writing this in differential notation motivates the SDE

$$
\boxed{
dX_t = b(t,X_t)\,dt + \sigma(t,X_t)\,dW_t, \qquad X_0=x.
}
$$

Here:
- $b: [0,T] \times \mathbb{R}^d \to \mathbb{R}^d$ is the **drift** (deterministic trend),
- $\sigma: [0,T] \times \mathbb{R}^d \to \mathbb{R}^{d \times m}$ is the **diffusion coefficient** (random fluctuation intensity),
- $W_t$ is an $m$-dimensional Brownian motion.

> **Important:** The "$dX_t$" notation is shorthand for the integral equation above. The SDE should always be understood in the integral sense.

### 1. Integral Form


The precise mathematical meaning of the SDE is given by the **integral equation**:

$$
X_t = X_0 + \int_0^t b(s,X_s)\,ds + \int_0^t \sigma(s,X_s)\,dW_s, \quad \forall t \in [0,T].
$$

The first integral is a **Riemann or Lebesgue integral**, while the second is an **Itô stochastic integral**.

### 2. Regularity Conditions


For the integrals to be well-defined, we typically require:

$$
\int_0^T |b(s,X_s)|\,ds < \infty \quad \text{a.s.}
$$

$$
\int_0^T \|\sigma(s,X_s)\|^2\,ds < \infty \quad \text{a.s.}
$$

where $\|\cdot\|$ denotes the Frobenius norm for matrices.

---

## What Is a Solution?


There are several notions of solution; the distinction is subtle but important.

### 1. Strong Solution


**Definition:** A process $X_t$ is a **strong solution** of

$$
dX_t=b(t,X_t)\,dt+\sigma(t,X_t)\,dW_t, \quad X_0 = x
$$

on $[0,T]$ if:

1. $X_t$ is **adapted** to the filtration $\{\mathcal{F}_t\}$ generated by $W_t$,
2. The integrability conditions hold:
   $$
   \int_0^T |b(t,X_t)|\,dt < \infty \quad \text{and} \quad \int_0^T \|\sigma(t,X_t)\|^2\,dt < \infty \quad \text{a.s.}
   $$
3. The integral equation holds for all $t\in[0,T]$ almost surely.

**Key point:** A strong solution is constructed on the **given** probability space with the **given** Brownian motion.

### 2. Weak Solution


**Definition:** A **weak solution** consists of a probability space $(\Omega,\mathcal{F},\mathbb{P})$, a filtration $\{\mathcal{F}_t\}$, a Brownian motion $W_t$, and a process $X_t$ such that the integral equation holds.

**Key difference:** We are allowed to **choose** the probability space and Brownian motion. Weak existence is a distributional property.

### 3. Pathwise Uniqueness vs. Uniqueness in Law


- **Pathwise uniqueness:** If $X_t$ and $Y_t$ are two strong solutions on the same space with the same Brownian motion and $X_0 = Y_0$, then $X_t = Y_t$ for all $t$ a.s.

- **Uniqueness in law:** Any two solutions have the same law (probability distribution).

**Theorem (Yamada-Watanabe):** 
1. Pathwise uniqueness + weak existence $\Rightarrow$ strong uniqueness.
2. Strong uniqueness $\Rightarrow$ pathwise uniqueness and uniqueness in law.

### 4. Connection to Martingale Problem


The **martingale problem** (Stroock-Varadhan) characterizes weak solutions via the infinitesimal generator. For the SDE above, we seek a process $X_t$ such that for all $f \in C_c^2(\mathbb{R}^d)$,

$$
M_t^f := f(X_t) - f(X_0) - \int_0^t \mathcal{L}f(X_s)\,ds
$$

is a martingale, where

$$
\mathcal{L}f = \sum_{i=1}^d b^i \frac{\partial f}{\partial x^i} + \frac{1}{2}\sum_{i,j=1}^d (\sigma\sigma^T)^{ij} \frac{\partial^2 f}{\partial x^i \partial x^j}
$$

is the **infinitesimal generator**.

---

## Itô's Lemma for SDEs


If $X_t$ solves

$$
dX_t=b(t,X_t)\,dt+\sigma(t,X_t)\,dW_t
$$

and $f \in C^{1,2}([0,T] \times \mathbb{R}^d)$, then **Itô's lemma** gives

$$
\boxed{
df(t,X_t)
=
\left(\frac{\partial f}{\partial t} + \sum_{i=1}^d b^i \frac{\partial f}{\partial x^i} + \frac{1}{2}\sum_{i,j=1}^d (\sigma\sigma^T)^{ij} \frac{\partial^2 f}{\partial x^i \partial x^j}\right)(t,X_t)\,dt
+ \sum_{j=1}^m \sum_{i=1}^d \sigma^{ij} \frac{\partial f}{\partial x^i}(t,X_t)\,dW_t^j.
}
$$

**Compact notation:** Using the generator $\mathcal{L}$,

$$
df(t,X_t) = \left(\frac{\partial f}{\partial t} + \mathcal{L}f\right)(t,X_t)\,dt + \nabla f \cdot \sigma(t,X_t)\,dW_t.
$$

### 1. Scalar Case


For $d=m=1$, this simplifies to:

$$
df(t,X_t) = \left(f_t + b f_x + \frac{1}{2}\sigma^2 f_{xx}\right)(t,X_t)\,dt + \sigma f_x(t,X_t)\,dW_t.
$$

### 2. Applications of Itô's Lemma


Itô's lemma is the fundamental tool for:

1. **Finding explicit solutions** via transformations
2. **Deriving properties** of solutions (moments, distributions)
3. **Constructing martingales** for pricing and probability
4. **Connecting SDEs to PDEs** via Feynman-Kac
5. **Change of variables** in stochastic calculus

---

## Canonical Examples


### 1. Brownian Motion with Drift


$$
dX_t=\mu\,dt+\sigma\,dW_t
\quad\Rightarrow\quad
X_t = X_0 + \mu t + \sigma W_t.
$$

This is a **Gaussian process** with:
- Mean: $\mathbb{E}[X_t] = X_0 + \mu t$
- Variance: $\text{Var}[X_t] = \sigma^2 t$

### 2. Geometric Brownian Motion (GBM)


$$
\boxed{
dS_t = \mu S_t\,dt + \sigma S_t\,dW_t.
}
$$

**Solution via Itô's lemma:** Apply Itô to $f(S) = \log S$:

$$
d(\log S_t) = \frac{1}{S_t}dS_t - \frac{1}{2}\frac{1}{S_t^2}(dS_t)^2 = \left(\mu - \frac{\sigma^2}{2}\right)dt + \sigma dW_t
$$

Integrating:

$$
\log S_t = \log S_0 + \left(\mu - \frac{\sigma^2}{2}\right)t + \sigma W_t
$$

Therefore:

$$
\boxed{
S_t = S_0\exp\left(\left(\mu-\frac{\sigma^2}{2}\right)t + \sigma W_t\right).
}
$$

**Properties:**
- $S_t$ is **log-normal**: $\log S_t \sim \mathcal{N}\left(\log S_0 + (\mu - \sigma^2/2)t, \sigma^2 t\right)$
- $\mathbb{E}[S_t] = S_0 e^{\mu t}$
- $\text{Var}[S_t] = S_0^2 e^{2\mu t}(e^{\sigma^2 t} - 1)$
- GBM is the foundation of the **Black-Scholes model**

### 3. Ornstein-Uhlenbeck (OU) Process


$$
\boxed{
dX_t = \kappa(\theta - X_t)\,dt + \sigma\,dW_t.
}
$$

This is a **mean-reverting** Gaussian diffusion with:
- **Mean reversion speed:** $\kappa > 0$
- **Long-term mean:** $\theta$
- **Volatility:** $\sigma$

**Solution:** Using integrating factor $e^{\kappa t}$:

$$
\boxed{
X_t = X_0 e^{-\kappa t} + \theta(1 - e^{-\kappa t}) + \sigma \int_0^t e^{-\kappa(t-s)}\,dW_s.
}
$$

**Properties:**
- $\mathbb{E}[X_t] = X_0 e^{-\kappa t} + \theta(1 - e^{-\kappa t}) \to \theta$ as $t \to \infty$
- $\text{Var}[X_t] = \frac{\sigma^2}{2\kappa}(1 - e^{-2\kappa t}) \to \frac{\sigma^2}{2\kappa}$ as $t \to \infty$
- **Stationary distribution:** $\mathcal{N}\left(\theta, \frac{\sigma^2}{2\kappa}\right)$

### 4. Linear SDE (General Form)


$$
dX_t = (a(t) + b(t)X_t)\,dt + (c(t) + d(t)X_t)\,dW_t
$$

where $a, b, c, d$ are deterministic functions. These can be solved explicitly using **variation of constants**.

---

## Existence and Uniqueness


### 1. Standard Conditions


**Theorem (Existence and Uniqueness):** Suppose $b$ and $\sigma$ satisfy:

1. **Lipschitz condition:** There exists $K > 0$ such that for all $t \in [0,T]$ and $x, y \in \mathbb{R}^d$,
   $$
   |b(t,x) - b(t,y)| + \|\sigma(t,x) - \sigma(t,y)\| \leq K|x - y|
   $$

2. **Linear growth condition:** There exists $C > 0$ such that for all $t \in [0,T]$ and $x \in \mathbb{R}^d$,
   $$
   |b(t,x)| + \|\sigma(t,x)\| \leq C(1 + |x|)
   $$

Then for any $\mathcal{F}_0$-measurable $X_0$ with $\mathbb{E}[|X_0|^2] < \infty$, there exists a **unique strong solution** $X_t$ on $[0,T]$.

### 2. Proof Sketch


The proof uses a **Picard iteration** (fixed-point argument):

1. Define $X_t^{(0)} = X_0$
2. Iterate: $X_t^{(n+1)} = X_0 + \int_0^t b(s, X_s^{(n)})\,ds + \int_0^t \sigma(s, X_s^{(n)})\,dW_s$
3. Show $X_t^{(n)}$ converges in $L^2$ using Lipschitz and growth conditions
4. The limit $X_t$ is the unique solution

**Key estimates:**
- **Itô isometry:** $\mathbb{E}\left[\left|\int_0^t H_s\,dW_s\right|^2\right] = \mathbb{E}\left[\int_0^t |H_s|^2\,ds\right]$
- **Grönwall's inequality:** Controls exponential growth

### 3. Examples of Non-uniqueness


When Lipschitz fails, uniqueness can fail.

**Example:** Consider $dX_t = \text{sgn}(X_t)\sqrt{|X_t|}\,dW_t$ with $X_0 = 0$.

Then $X_t \equiv 0$ is a solution, but there are also non-trivial solutions. This SDE has **weak existence but not pathwise uniqueness**.

### 4. Local Lipschitz and Explosion


If $b, \sigma$ are only **locally Lipschitz**, we get a unique solution up to a **stopping time** $\tau$ (explosion time).

**Example:** $dX_t = X_t^2\,dt + dW_t$ may explode in finite time.

---

## Simulation: Euler-Maruyama Scheme


SDEs rarely have closed-form solutions, so numerical simulation is essential.

### 1. The Scheme


For

$$
dX_t=b(t,X_t)\,dt+\sigma(t,X_t)\,dW_t,
$$

the **Euler-Maruyama scheme** on a grid $t_n=n\Delta t$ is

$$
\boxed{
X_{n+1} = X_n + b(t_n,X_n)\,\Delta t + \sigma(t_n,X_n)\,\Delta W_n,
}
$$

with $\Delta W_n \sim \mathcal{N}(0,\Delta t)$ independent.

### 2. Convergence Properties


**Theorem:** Under Lipschitz and growth conditions, the Euler-Maruyama scheme has:

1. **Strong convergence:** $\mathbb{E}[|X_T - X_T^{\Delta t}|] = O(\sqrt{\Delta t})$
2. **Weak convergence:** $|\mathbb{E}[f(X_T)] - \mathbb{E}[f(X_T^{\Delta t})]| = O(\Delta t)$ for smooth $f$

Higher-order schemes (Milstein, Runge-Kutta) achieve better convergence rates.

### 3. Python Implementation: GBM


```python
import numpy as np
import matplotlib.pyplot as plt

def simulate_GBM_EM(S0, mu, sigma, T, N, num_paths):
    """
    Simulate Geometric Brownian Motion using Euler-Maruyama.
    
    Parameters:
    -----------
    S0 : float
        Initial stock price
    mu : float
        Drift coefficient
    sigma : float
        Volatility
    T : float
        Time horizon
    N : int
        Number of time steps
    num_paths : int
        Number of sample paths
    
    Returns:
    --------
    t : array
        Time grid
    S : array
        Simulated paths (num_paths x N+1)
    """
    dt = T / N
    t = np.linspace(0, T, N+1)
    
    S = np.zeros((num_paths, N+1))
    S[:, 0] = S0
    
    for p in range(num_paths):
        dW = np.sqrt(dt) * np.random.randn(N)
        for n in range(N):
            S[p, n+1] = S[p, n] + mu * S[p, n] * dt + sigma * S[p, n] * dW[n]
    
    return t, S

# Example usage
T = 1.0
N = 500
S0 = 100.0
mu = 0.1
sigma = 0.2
num_paths = 20

t, S = simulate_GBM_EM(S0, mu, sigma, T, N, num_paths)

fig, ax = plt.subplots(figsize=(10, 6))
for p in range(num_paths):
    ax.plot(t, S[p], alpha=0.6)
ax.set_title("Geometric Brownian Motion (Euler-Maruyama)", fontsize=14)
ax.set_xlabel("Time t")
ax.set_ylabel("Stock Price $S_t$")
ax.grid(True, alpha=0.3)
ax.axhline(S0, color='black', linestyle='--', alpha=0.5, label='$S_0$')
ax.legend()
plt.tight_layout()
plt.show()
```

### 4. Exact Simulation for GBM


For GBM, we can simulate **exactly** using the analytical solution:

```python
def simulate_GBM_exact(S0, mu, sigma, T, N, num_paths):
    """Exact simulation of GBM using the analytical solution."""
    dt = T / N
    t = np.linspace(0, T, N+1)
    
    S = np.zeros((num_paths, N+1))
    S[:, 0] = S0
    
    for p in range(num_paths):
        W = np.cumsum(np.concatenate([[0], np.sqrt(dt) * np.random.randn(N)]))
        S[p, :] = S0 * np.exp((mu - 0.5 * sigma**2) * t + sigma * W)
    
    return t, S
```

This is more accurate than Euler-Maruyama for the same computational cost.

---

## Transformations and Change of Variables


### 1. Lamperti Transform


For SDEs of the form $dX_t = b(X_t)\,dt + \sigma(X_t)\,dW_t$, the **Lamperti transform**

$$
Y_t = \int_{X_0}^{X_t} \frac{du}{\sigma(u)}
$$

converts the SDE to one with **unit diffusion**:

$$
dY_t = \tilde{b}(Y_t)\,dt + dW_t
$$

where $\tilde{b}(y) = \frac{b(x) - \frac{1}{2}\sigma(x)\sigma'(x)}{\sigma(x)}$ evaluated at $x = \sigma^{-1}(y)$.

### 2. Example: CIR to Bessel Process


The CIR model $dr_t = \kappa(\theta - r_t)\,dt + \sigma\sqrt{r_t}\,dW_t$ can be transformed via $X_t = \sqrt{r_t}$ to a Bessel-like process.

---

## Multidimensional SDEs


For $X_t \in \mathbb{R}^d$ driven by $W_t \in \mathbb{R}^m$:

$$
dX_t^i = b^i(t, X_t)\,dt + \sum_{j=1}^m \sigma^{ij}(t, X_t)\,dW_t^j, \quad i = 1, \ldots, d
$$

The **correlation structure** is determined by the diffusion matrix $a = \sigma\sigma^T$:

$$
d\langle X^i, X^j \rangle_t = a^{ij}(t, X_t)\,dt
$$

**Example (Heston model):**

$$
\begin{cases}
dS_t = \mu S_t\,dt + \sqrt{V_t} S_t\,dW_t^1 \\
dV_t = \kappa(\theta - V_t)\,dt + \xi\sqrt{V_t}\,dW_t^2
\end{cases}
$$

with $d\langle W^1, W^2 \rangle_t = \rho\,dt$ (correlation $\rho$).

---

## Connection to PDEs


### 1. Feynman-Kac Formula


If $X_t$ solves $dX_t = b(X_t)\,dt + \sigma(X_t)\,dW_t$ and $u(t,x)$ satisfies

$$
\frac{\partial u}{\partial t} + b(x)\frac{\partial u}{\partial x} + \frac{1}{2}\sigma^2(x)\frac{\partial^2 u}{\partial x^2} = 0
$$

with terminal condition $u(T,x) = \phi(x)$, then

$$
u(t,x) = \mathbb{E}[\phi(X_T) | X_t = x].
$$

This connects **parabolic PDEs** to **expectations of diffusions**.

### 2. Kolmogorov Equations


- **Backward equation:** Describes evolution of expectations $u(t,x) = \mathbb{E}[f(X_T)|X_t = x]$
- **Forward equation (Fokker-Planck):** Describes evolution of densities $p(t,x)$ of $X_t$

---

## Summary


- An SDE is an **integral equation driven by Brownian motion**, written in differential notation for convenience.
- **Strong solutions** are constructed on a given probability space; **weak solutions** allow changing the space.
- **Itô's lemma** is the chain rule for stochastic calculus, connecting SDEs to their solutions and to PDEs.
- **Existence and uniqueness** require Lipschitz and growth conditions; failure can lead to explosion or non-uniqueness.
- **Euler-Maruyama** provides a practical simulation method with $O(\sqrt{\Delta t})$ strong convergence.
- SDEs connect deeply to **infinitesimal generators**, **martingale problems**, and **PDE theory** via Feynman-Kac.

The theory of SDEs is the foundation for modern quantitative finance, statistical physics, and filtering theory.
